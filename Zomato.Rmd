---
title: "Scrapping zomato restaurant and reviews"
output: html_document
---

Ref : 
* https://www.computerworld.com/article/2971265/how-to-drive-a-web-browser-with-r-and-rselenium.html
* https://cran.r-project.org/web/packages/tryCatchLog/vignettes/tryCatchLog-intro.html

Notes :
* Latest version of chrome 75.0+ is compatible to latest Rselenium

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Import necessary libraries

```{r import libraries, warning=FALSE}
suppressPackageStartupMessages({
  if (!require(rvest)) {library(rvest)} #for webscrapping
  if (!require(stringr)) {library(stringr)} #for string and regex functions
  if (!require(dplyr)) {library(dplyr)} #for dataframe operations
  if (!require(RSelenium)) {library(RSelenium)} #for selenium web driver
})
```

## Define zomato url and search area and fetch base url for Gachibowli restuarants

## This makes the script generic and usable to search for any other city + location pattern

```{r define search city and location}
search.city <- 'hyderabad'
url <- paste0('http://zomato.com/',search.city)
search.location <- 'Gachibowli'
```

## Use selenium web driver to navigate to zomato and search for any city and location

## Function to dynamically fetch the base url to work with.

```{r function to get search url, message=FALSE}
GetSearchUrl <- function() {
  driver <- rsDriver(browser=c("chrome"), port=4444L)
  browser <- driver[["client"]]
  
  browser$open()
  browser$navigate(url)
  
  #find search box and enter search area
  searchbox <- browser$findElement(using = 'id', "keywords_input")
  searchbox$sendKeysToElement(list(search.location))
  
  #Page takes a whole to to load notifications at top
  Sys.sleep(2)
  
  #Click on search button and print title
  searchbutton <- browser$findElement(using = 'id', "search_button")
  searchbutton$clickElement()
  
  title <- browser$getCurrentUrl()
  print(title)  #base search url
  
  # close browser and stop the selenium server
  browser$close()
  driver[["server"]]$stop()
  
  end.index <- regexpr('\\?', title)
  search.base.url <- substring(title,0,end.index-1)
  print(search.base.url)
  return(search.base.url)
}
```

## Define functions to clean data related to cusines, cost, timings and collections

```{r define functions to clean restaurants data}
cuisines.extractor <- function(x){
  ifelse(
    is.null(html_node(x, '.clearfix')), 
    NA,
    html_node(x, '.clearfix .col-s-11.col-m-12') %>% html_text()
  )
}

cost.extractor <- function(x){
  ifelse(
    is.null(html_node(x, '.res-cost.clearfix')), 
    NA,
    as.numeric(gsub(",", "", gsub("\u20b9", "", html_node(x, '.res-cost.clearfix .col-s-11.col-m-12') %>%
                                  html_text())))
  )
}

timings.extractor <- function(x){
  ifelse(
    is.null(html_node(x, '.res-timing.clearfix')), 
    NA,
    gsub("\n\\s+", "", html_node(x, '.res-timing.clearfix .col-s-11.col-m-12') %>% html_text())
  )
}

collections.extractor <- function(x){
  ifelse(
    is.null(html_node(x, '.res-collections.clearfix')), 
    NA,
    html_node(x, '.res-collections.clearfix .col-s-11.col-m-12') %>% html_text()
  )
}
```

## Start with search base url

### ?nearby=0 loads search results with pagination excluding nearby locations

### For any issues with selenium webdriver below code can be commented and gachibowli resturant url can be directly used 

```{r get base url, results="hide", message=FALSE}
# If selenium load fails due to driver issues, use base url directly
#search.base.url <- "https://www.zomato.com/hyderabad/gachibowli-restaurants"
search.base.url <- GetSearchUrl()
search.base.url <- paste0(search.base.url, "?nearby=0")
html.page <- search.base.url %>% read_html()
```

# Identify total number of pages in search results

```{r count search results}
pagination.div.text <- html.page %>% html_nodes('.pagination-number div') %>% html_text();
text.length <- nchar(pagination.div.text)
search.page.count <- strtoi(substring(pagination.div.text, 11, text.length-1))
```

# Create empty dataframe with desired columns to store restaurants data

```{r}
restaurant_list <- data.frame(matrix(ncol = 6, nrow = 0))
headers <- c("Name", "Cuisines", "Cost", "Timings", "Collections", "url")
colnames(restaurant_list) <- headers
```

# Iterate through each page and get resturant data

```{r}
#total pages - 41
for (i in 1:search.page.count) {
  page_url <- paste0(search.base.url, "&page=",i)
  print(page_url)
  restaurant.listing.page <- page_url%>% read_html()
  
  restaurant_name <- gsub("\n\\s+","", restaurant.listing.page %>% html_nodes('.fontsize0') %>% html_text())
  
  cuisines <- 
  restaurant.listing.page %>% 
  html_nodes('.search-page-text.clearfix.row') %>% 
  sapply(cuisines.extractor)

  cost <- 
    restaurant.listing.page %>% 
    html_nodes('.search-page-text.clearfix.row') %>% 
    sapply(cost.extractor)
  
  timings <- 
    restaurant.listing.page %>% 
    html_nodes('.search-page-text.clearfix.row') %>% 
    sapply(timings.extractor)
  
  collections <- 
    restaurant.listing.page %>% 
    html_nodes('.search-page-text.clearfix.row') %>% 
    sapply(collections.extractor)
  
  url <- restaurant.listing.page %>% html_nodes('.fontsize0') %>% html_attr('href')

  current_results = data.frame(restaurant_name, cuisines, cost, timings, collections, url)
  restaurant_list <- rbind(restaurant_list, current_results)
  Sys.sleep(15)
}
```

## Checkpoint (1)
## Write collected restaurant list to csv file to avoid losing list

```{r}
write.csv(restaurant_list, "restaurant_list.csv")
```

## reviewer id - each user is unique on zomato and using this, we can identify how old/new user is on zomato. Same can be used to navigate to user profile (even if users change their name)

## reviewer.reviews.count - Number of reviews posted by user. Can be used to identify trustness of reviews

## reviewer.followers - No of followers, probably can be considered more trusted reviews

## review.text - can be used for text analytics

## review.rating - important to scrap from analytics perspective

## review.photos - count of photos uploaded for this review. Make reviews more genuine

## review.time - time of review

```{r define function to read reviews}
ReadReviewsForResturant <- function(html.page) {
  reviews.df <- data.frame(matrix(ncol = 7, nrow = 0))
  headers <- c("UserId", "Review Text", "Rating", "Time", "Reviews Count", "Followers Count", "Photos Count")
  colnames(reviews.df) <- headers
  
  tryCatch({
    reviewer.id <- html.page %>% html_nodes('.res-large-snippet .header a') %>%
      html_attr('data-entity_id')
    review.metadata <- gsub("\n\\s+", "", html.page %>% 
                                         html_nodes('.res-large-snippet .grey-text') %>% 
                                         html_text())
    reviewer.reviews.count <- as.numeric(substring
                                         (review.metadata, 0, regexpr('Review', review.metadata)-1))
    
    reviewer.followers <- ifelse(
      regexpr(',', review.metadata) > 0,
      as.numeric(
        substring(review.metadata, (regexpr(
          ',', review.metadata)+1), (regexpr('Follower',review.metadata)-1))
        ),
      0)

    review.text <- substring(gsub("\n\\s+", "", html.page %>% html_nodes('.rev-text') %>% html_text()), 7)
    review.rating <- as.numeric(substring((html.page %>% 
                                             html_nodes('.rev-text div') %>% 
                                             html_attr('aria-label'))[c(TRUE, FALSE)], 7))
    
    review.photos <- c()
    review.body <- html.page %>% html_nodes('.res-review-body')
    no.of.reviews <- length(review.body)
    print(paste("No of reviews visible :", no.of.reviews))
    for(i in 1:no.of.reviews) {
      photos.count <- 0
      item <- review.body[i]
      visible.photos.count <- tryCatch ({
        length(item %>% html_nodes('.parentPhotoBox .js-heart-container'))
      }, error = function(e) {
        message("No photos available for review ! ")
      })
      print(paste(visible.photos.count,"Count .."))
      if(visible.photos.count >= 6) {
        additional.photos.count <- 0
        additional.photos.node <- tryCatch ({
          item %>% html_nodes('.overlay.res-photo-thumbnail')
        }, error = function(e) {
          message("Error in locating additional photos !")
        })
        if(length(additional.photos.node) > 0) {
          additional.photos.text <- gsub("\n\\s+", "", additional.photos.node %>% html_text())
          additional.photos.count <- as.numeric(
            substring(additional.photos.text, 2, (regexpr('photos', additional.photos.text)-2))
          ) 
        }
        photos.count <- visible.photos.count + additional.photos.count
      }
      else {
        photos.count <- visible.photos.count
      }
      review.photos[i] <- photos.count
    }
    
    review.time <- html.page %>% html_nodes('time') %>% html_attr('datetime')
    
    current.chunk <- data.frame(reviewer.id,
                                review.text,
                                review.rating,
                                review.time,
                                reviewer.reviews.count,
                                reviewer.followers,
                                review.photos)
    names(current.chunk) <- headers
    reviews.df <- rbind(reviews.df, current.chunk)
  }, error = function(NoSuchElementException) {
    message("No reviews available, skip !")
  })
  return(reviews.df)
}

```

#Click on load more n times, to load all reviews from page

```{r define function to load desired number of reviews in single page}
LoadAllRestaurantReviews <- function(url) {
  driver <- rsDriver(browser=c("chrome"), port=4444L)
  browser <- driver[["client"]]

  browser$open()  #open client and navigate to restaurant reviews url
  browser$navigate(url)

  Sys.sleep(2)
  
  all.reviews.link <- tryCatch({
    browser$findElement(using = 'xpath', "//*[contains(text(), 'All Reviews')]")
  }, error = function(e) {
    message("No reviews available, skip !")
    browser$close()
    driver[["server"]]$stop()
    return(NULL)
  })
  if(!is.null(all.reviews.link)) {
    all.reviews.link$clickElement()
    
    #Page takes a while to to load
    Sys.sleep(2)
  
    #Load max 100 reviews
    tryCatch({
      for(i in 1:9) {
          load.more.button <- tryCatch({
            browser$findElement(using = 'class', "zs-load-more-count")
          }, error = function(NoSuchElementException) {
            message("All reviews loaded, continue !")
            break()
          })
          if(!is.null(load.more.button)) {
            load.more.button$clickElement()
            Sys.sleep(10)
          }
      }
    }, error = function(NoSuchElementException) {
       message("All reviews loaded or reviews not present, continue !")
    }) 
    
    page_source <- browser$getPageSource() # read in page contents
    html.page <- read_html(page_source[[1]])
    browser$close()
    
    # stop the selenium server
    driver[["server"]]$stop()
    
    return(html.page)
  }
}
```

# iterate through each restaurant and fetch reviews and users metadata and write to csv 

```{r echo=T, results='hide'}
system.time({
  restaurant.list <- read.csv("restaurant_list.csv")
  
  restaurant.reviews.df <- data.frame(matrix(ncol = 13, nrow = 0))
  headers <- c("Name", "Cuisines", "Cost", "Timings", "Collections", "url",
               "UserId", "Review Text", "Rating", "Time", "Reviews Count", 
               "Followers Count", "Photos Count")
  colnames(restaurant.reviews.df) <- headers
  
  for(row in 1:nrow(restaurant.list)) { #nrow(restaurant.list) #470
    tryCatch ({
      print(paste("Fetching reviews for :", restaurant.list[row, "url"]))
      url <- paste0(restaurant.list[row, "url"],"/reviews")
      restaurant.name <- restaurant.list[row, "restaurant_name"]
      restaurant.page <- LoadAllRestaurantReviews(url)
      if(!is.null(restaurant.page)) {
        system.time({
          reviews.df <- ReadReviewsForResturant(restaurant.page)
          write.csv(reviews.df, paste0(restaurant.name, ".csv"))
        })
      }
      #break()
      Sys.sleep(10)
    }, error = function(e) {
      message("Error in processing, continue to next restaurant !")
    })
  }
})
```